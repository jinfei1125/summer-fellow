# summer-fellow
This repository contains some exploration for my data engineering fellowship at HeRop (Healthy Regions &amp; Policies Lab)

## SQL-style Database Provider Comparison

| Provider   | Cost | Advantages | Limitations | Note | Compatibility with PostGeoDa |
| -- | ------ | ---- | ---- | - |------|
| Planetscale | - Free Tier: 3 free databases, 10GB storage, 100 million rows read/mo, 10 million rows written/mo (per database) <br/>- $1.25 per 1GB storage, $15/mo per 100 million rows read, $15/mo per 10 million rows written | - Serverless <br/> - Fast (reads 1000 rows/s, writes 200 rows/s) <br/>- Built on MySQL and Vitess (a database clustering system for horizontal scaling of MySQL, powering Slack, Square, GitHub, YouTube, enterprise reliability) <br/>- Scale indefinitely <br/>- Branch feature: making non-blocking schema changes possible <br/>- Online GUI and local CLI <br/>- Could be in free tier? | - only mySQL compatible <br/>- focusing on branching/testing schema instead of just storing  | [Docs](https://docs.planetscale.com/) (I feel the schema of our database won't change often?)| only mySQL compatible |
| yugabyteDB |Free tier: (Cluster: One three-node YugabyteDB cluster Storage: 5 GB High availability: Across multiple availability zones Client APIs: YSQL and YCQL Administration: Yugabyte Cloud Console) Customized pricing plan needs email contact (no reply yet)| - Could Native (AWS, Google Cloud, Azure, Pivotal, Docker, Kubernetes) - High performance (Single-digit millisecond latency) <br/>- auto sharding and rebalancing (planetscale's branching) <br/>- global resilience for future replica <br/>- internet-scale apps <br/>- reduced infrastructure and software licensing costs <br/>- Integration w/ Apache Kafka, Spark, KSQL <br/>- Open Source <br/>- PostgreSQL compatible <br/>- Enterprise feature (read replicas, security, distributed backups, change data capture) | - only four deployment (macOS, linux, kubernetes, docker) <br/>- need provide email to know the pricing, and free tier is still in beta |[Compare yugabyte with other database](https://docs.yugabyte.com/latest/comparisons/) | Postgeoda extensions [not supported](https://docs.yugabyte.com/latest/api/ysql/extensions/)|
| Supabase | Free tier: <br/>- 500M database space <br/>- unlimited API requests <br/>- paused after 1 week inactivity <br/>- 10,000 users w/ 1hour audit trail <br/>- 1 GB storage total, 2GB tranfer limit <br/> <br/> Pro: - $25/project/month - 8 GB database space - 100,000 Users - 100 GB storage total, 200 GB tranfer limit Pay as you go: - $25/project/month (plus usage costs) - $0.125 per GB/project/month - Unlimited users - $0.021 per GB storage total, $0.07 per GB tranfer limit | - PostgreSQL database, works natively with Postgres extensions —PostGIS supported - Built-in Authentications (controls exactly what users can access) - Availability: real-time enabled - Spreadsheet like editing as well as sql editor - Provides instant APIs and handles CRUD for users (Python library coming soon) - Connected to Github | - Relatively small database for free and pro tier users - Possibly we have to choose "Pay as you go" plan | Docs: [https://supabase.io/database](https://supabase.io/database) Non-fit with our needs: no strict authentication and real-time needs for open-source data archive (though now using API need application, I assume this application process will be canceled in future) Fit with our project: - instant API (though our API is already finished) - PostGIS supported | It only has several pre-installed postgres extensions (including PostGIS). But writing a script in SupaScript with Javascript and move it to database sounds feasible! (From this link: https://github.com/supabase/supabase/discussions/679#discussioncomment-763161) —if we only use certain postgeoda function, or maybe we can rewrite them in JS if possible|
| Hasura | Free tier: - 1 GB data pass-through per month (request data that is sent to upstream services (eg: databases) and response data sent to downstream services (eg: web apps)) - 60 requests/minute Standard $99/mo/project: - 20 GB data pass-through ($2 per additional GB) - Unlimited requests - High Availability Customized enterprise plan | - Build apps and GraphQL APIs 10x faster - Built-in autorization and catching - 5x-50x more performant than hand0rolled APIs - Support PostgreSQL, MySQL, SQL Server, AWS Aurora, Google BigQuery, Oracle, mongoDB, etc. - linear scalability - With query size increase, comparable advantage over NodeJS increase - Widely used: McKinsey, netlify, Walmart, etc | - Charge via data passthrough, instead of data storage <br/>-Relatively expensive | Docs: [https://hasura.io/docs/latest/graphql/core/index.html](https://hasura.io/docs/latest/graphql/core/index.html) - API feature is attractive| Not support PostGeoDa extension's [SQL function](https://hasura.io/docs/latest/graphql/core/databases/postgres/schema/custom-functions.html#supported-sql-functions)|
| cockroach | Free tier: with most features and geospatial data types supported Customized plan pricing needs contact|- Spatial data types - Scalability - Data Resistence - Native JSON Support - Active dynamic schema changes - Used by Ebay, DoorDash, Bose, Comcast | - lower throughput and higher latency (compared to yugabyteDB) - Less fit for internet-scale OLTP and no enterprise features - Cheaper than AWS Aurora | Compare different plan: [https://www.cockroachlabs.com/compare/](https://www.cockroachlabs.com/compare/) Compare to Aurora [https://www.cockroachlabs.com/blog/aurora-price-vs-cockroachdb/](https://www.cockroachlabs.com/blog/aurora-price-vs-cockroachdb/)| Stored procedures and functions are [not supported in cockroach](https://www.cockroachlabs.com/docs/v21.1/postgresql-compatibility)|
|Google Spanner| Regional pricing (USD): single-region $0.90/node/hr, Multi-region pricing: $3.00/node/hr | -up to 99.999% availability | - Cloud lock-in - Not postgreSQL-compatible - Less cost effective - Less fit for internet-scale OLTP - relatively small data density | | Not supported PostGeoDa function. We can install only the extensions that are [supported by Cloud SQL](https://cloud.google.com/sql/docs/postgres/extensions#postgresql-extensions-supported-by-cloud-sql), which are only four types: PostGIS extensions, Data type extensions, Language extensions, and Miscellaneous extensions |
|Amazon Aurora | starting at $0.082/hour (for db.t3.medium) |PostgreSQL-compatible enterprise-class database. - We can create a database with RDS console or import from S3. | - Cloud lock-in - Nonlinear write scalability - relatively long latency - less fit for internet-scale online transaction processing - limited database instances options | |Only support certain number of [Postgres extensions](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Extensions.html#AuroraPostgreSQL.Extensions.12), not including PostGeoda. PostGIS supported|
|Amazon RDS for postgreSQL |$0.072/hour for db.t3.medium |  a more economical, less intensive option (up to 5 replicas instead of 15 of Aurora) compared to with more database instance options.  | - Cloud lock-in -Less replicable than Aurora| (An article comparing RDS and Aurora: https://aws.amazon.com/blogs/database/is-amazon-rds-for-postgresql-or-amazon-aurora-postgresql-a-better-choice-for-me/) | PostGeoda is not in the [supported or trusted extensions lists](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.FeatureSupport.Extensions.13x)|
|Vitess|  | - used by planetscale - automated sharding solution for MySQL | - Lack of continuous availability - no single logical SQL database: developers have to be acutely aware of their sharding mechanism while designing their schema as well as while executing queries - mainly for mySQL| | only for MySQL|
# Comparison of data storage format
## Parquet

- If we are trying to read Parquet file with Lambda, we need to use the AWS Data Wrangler package developed by AWS (https://github.com/awslabs/aws-data-wrangler)

- AWS Athena (a serverless and interactive query service, no infrastructure to set up) runs 34x faster and cost 99.7% less when scanning data stored in Apache Parquet format (130GB) than data stored as text files (https://aws.amazon.com/blogs/big-data/analyzing-data-in-s3-using-amazon-athena/)

- We could use PySpark script, running on Amazon EMR, to convert CSV, TSV, JSON or any Hive supported format to Parquet (https://github.com/amazon-archives/aws-big-data-blog/tree/master/aws-blog-spark-parquet-conversion) A 2-node r3.x8large cluster in US-east was able to convert 1 TB of log files into 130 GB of compressed Apache Parquet files (87% compression) with a total cost of $5)

- Also good to read with Apache Spark kernel and read as a spark dataframe and conduct other analysis for research (example PySpark code spark.read.parquet('s3://amazon-reviews-pds/parquet/product_category=Books/*.parquet') )

